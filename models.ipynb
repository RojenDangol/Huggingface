{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNt/endDG5S2Tt65yHEF0xf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kubU32opxE8s","executionInfo":{"status":"ok","timestamp":1750668141110,"user_tz":-345,"elapsed":651,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"02a3c5a2-ba9d-45dc-803b-10ba78bba6da"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["from transformers import AutoTokenizer, BertModel\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","model = BertModel.from_pretrained(\"bert-base-cased\")\n","\n","encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n","print(encoded_input)"]},{"cell_type":"code","source":["tokenizer.decode(encoded_input[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"wCEN5P_9x0XK","executionInfo":{"status":"ok","timestamp":1750668145918,"user_tz":-345,"elapsed":13,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"9c47a4a3-f504-41d7-f67b-b361e34485ed"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"[CLS] Hello, I ' m a single sentence! [SEP]\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")\n","print(encoded_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIcq1UxuyVPd","executionInfo":{"status":"ok","timestamp":1750665521036,"user_tz":-345,"elapsed":507,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"79090f3c-f009-4562-fd90-b92869350dc4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,  146,  112,  182, 2503,  117, 6243,\n","         1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}]},{"cell_type":"markdown","source":["**Padding inputs**\n","\n","If we ask the tokenizer to pad the inputs, it will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one:"],"metadata":{"id":"dJyhHJulziCY"}},{"cell_type":"code","source":["encoded_input = tokenizer([\"How are you?\", \"I'm fine, thank you!\"],padding=True, return_tensors=\"pt\")\n","print(encoded_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15cYM3qvytnt","executionInfo":{"status":"ok","timestamp":1750665592453,"user_tz":-345,"elapsed":12,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"b1262022-f250-4e26-cd34-a0b5aef7f507"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n","        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}]},{"cell_type":"markdown","source":["The tensors might get too big to be processed by the model. For instance, BERT was only pretrained with sequences up to 512 tokens, so it cannot process longer sequences. If you have sequences longer than the model can handle, youâ€™ll need to truncate them with the truncation parameter:"],"metadata":{"id":"SSV_gOWFz2tS"}},{"cell_type":"code","source":["encoded_input = tokenizer(\n","    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n","    truncation=True,\n",")\n","print(encoded_input[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8AlHTmZz6g3","executionInfo":{"status":"ok","timestamp":1750665789653,"user_tz":-345,"elapsed":11,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"2ef6402f-924e-4061-bf94-18362cd4fa30"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]\n"]}]},{"cell_type":"markdown","source":["By combining the padding and truncation arguments, you can make sure your tensors have the exact size you need:"],"metadata":{"id":"G7hckmbf0FbZ"}},{"cell_type":"code","source":["encoded_input = tokenizer(\n","    [\"How are you?\", \"I'm fine, thank you!\"],\n","    padding=True,\n","    truncation=True,\n","    max_length=5,\n","    return_tensors=\"pt\",\n",")\n","print(encoded_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzOsiK3O0Ez-","executionInfo":{"status":"ok","timestamp":1750665846478,"user_tz":-345,"elapsed":13,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"6538f34d-2dfc-4c69-83d2-a2f06da924af"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1731, 1132, 1128,  102],\n","        [ 101,  146,  112,  182,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1]])}\n"]}]},{"cell_type":"code","source":["sequences = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]"],"metadata":{"id":"fa_NdX460qHz","executionInfo":{"status":"ok","timestamp":1750665963744,"user_tz":-345,"elapsed":42,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["encoded_sequences = [\n","    [\n","        101,\n","        1045,\n","        1005,\n","        2310,\n","        2042,\n","        3403,\n","        2005,\n","        1037\n","    ],\n","    [101, 1045, 5223, 2023, 2061, 2172, 999, 102],\n","]"],"metadata":{"id":"Lj5UUJqr0rGV","executionInfo":{"status":"ok","timestamp":1750666082400,"user_tz":-345,"elapsed":7,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)\n","output = model(model_inputs)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rBDpN6t0tXb","executionInfo":{"status":"ok","timestamp":1750666306121,"user_tz":-345,"elapsed":238,"user":{"displayName":"Rojen Dangol","userId":"03610102197767751424"}},"outputId":"721125c4-c600-4e71-e49d-7bfa39bc494e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0540,  0.0534,  0.1497,  ..., -0.4506,  0.4883, -0.0358],\n","         [-0.2622, -0.1387,  0.6599,  ..., -0.3344,  0.6585,  0.0504],\n","         [-0.1086, -0.3388,  0.6278,  ..., -0.3018,  0.6292,  0.0722],\n","         ...,\n","         [-0.2101,  0.2066,  0.2913,  ..., -0.4580,  0.1699,  0.2519],\n","         [-0.2638,  0.1462,  0.4753,  ..., -0.7680,  0.3537,  0.0579],\n","         [-0.2006, -0.2844,  0.9313,  ..., -0.5717,  0.7596,  0.3495]],\n","\n","        [[-0.0725,  0.0540, -0.0037,  ...,  0.1450,  0.2381, -0.0164],\n","         [-0.1619, -0.3062, -0.2282,  ...,  0.3782, -0.1170,  0.1295],\n","         [-0.1173, -0.1003,  0.1703,  ...,  0.3227, -0.1996,  0.1646],\n","         ...,\n","         [-0.1858,  0.0967,  0.4599,  ...,  0.3619,  0.1154,  0.1746],\n","         [ 0.0969, -0.2961,  0.0700,  ..., -0.0877, -0.1499,  0.1670],\n","         [ 0.2359, -0.3709, -0.1987,  ...,  0.6541,  0.7405, -0.4106]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.4584,  0.3363,  0.9988,  ...,  0.9998, -0.7049,  0.9967],\n","        [-0.7491,  0.3085,  0.9996,  ...,  0.9998, -0.8474,  0.9730]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"]}]}]}